{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ZEfyj3G7T2n2BbH11TGDKSp80i98Vltt",
      "authorship_tag": "ABX9TyMCr4rOaaBL99f/QpaHrqFZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dskreddy15/Projects/blob/main/ResumeChatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfwP4q_hGNHL",
        "outputId": "1d5395dc-3a3b-4466-fce9-40deed1f2e57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install newspaper3k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5gmJC6zGTSb",
        "outputId": "08c9f5f8-1077-40fc-b61b-4b14243977d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/211.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/211.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.11.2)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.1)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.3)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.31.0)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-3.5.0-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.2/97.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2023.7.22)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.12.2)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13541 sha256=431101272144cb122a43e8add0332ebd36943fcbccdfbb1ae65c22fecf974fb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3339 sha256=50170df810701bd05df308c31953c70f18cf8850f4f186d9fd80f005acb6de43\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=acbbde21bb605170a562c0c9cdf6dc886e2fa021fb8e510838db2806523e80e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=4c63f90bde7f1df722ab88ccfdc56f014af33c0631b66df31f5206ee3ffc1953\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from newspaper import Article\n",
        "import random\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "rkK9FshEGbGE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0kLZIjvHdSC",
        "outputId": "1fc7317e-6db7-48ca-c76a-65065963c49d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7aauEHuSKZAr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1hQvLkhK5IP",
        "outputId": "0bf3d5d1-cd6e-4be6-e4b3-c0791391d5bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX7G5rtVMvG3",
        "outputId": "76def119-89f7-431e-9a53-924b50f3dbeb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/232.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "pdf_file_path = '/content/drive/MyDrive/FreshDhatriResume.pdf'\n",
        "\n",
        "# Open the PDF file\n",
        "pdf_file = open(pdf_file_path, 'rb')\n",
        "\n",
        "# Create a PDF reader object\n",
        "pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "# Initialize an empty string to store the text\n",
        "text = ''\n",
        "\n",
        "# Iterate through each page and extract text\n",
        "for page_num in range(len(pdf_reader.pages)):\n",
        "    page = pdf_reader.pages[page_num]\n",
        "    text += page.extract_text()\n",
        "\n",
        "# Close the PDF file\n",
        "pdf_file.close()\n",
        "\n",
        "# Now 'text' contains the extracted text from the PDF\n"
      ],
      "metadata": {
        "id": "JCR18aLnMm67"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        },
        "id": "6lyLuGPQN9oI",
        "outputId": "71ad4b55-3235-4470-fd56-21aa7f081dca"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DHATRI SAI KUMAR REDDY GUDIPELLY\\n+1(858) 923-3101 ⋄San Marcos, CA\\ndskreddy15@gmail.com ⋄linkedin.com/dhatrisaireddy ⋄github.com/dskreddy15\\nOBJECTIVE\\nLooking for a full-time position in a competitive setup where one will be able to contribute to the firm’s progress\\nsubstantially and learn new skills. Eager to apply research work to industry data and continue innovating and\\ndeveloping new ideas.\\nEDUCATION\\nMaster of Computer Science , California State University San Marcos 2023\\nRelevant Coursework: Machine Learning, Advance Database Management Systems, Artificial Intelligence, and Data\\nMining.\\nSKILLS\\nLanguages Python, JavaScript, SQL, C/C++, R\\nFrameworks ReactJs, Spring Boot, AngularJs, NodeJs, Tensorflow, Scikit\\nTools GIT, PostgreSQL, MySQL, Sonarqube, Jupyter, Visual Studio\\nPlatforms Linux, Windows, Raspberry, AWS, GCP, arm Mbed, MATLAB\\nSkills Research, AI/ML,Python, SQL, Mathematics, Data Mining, Data Analysis, Statistics.\\nEXPERIENCE\\nStudent Assistant Aug 2021 - May 2023\\nCalifornia State University San Marcos, CA\\n•Assisting with course preparation, Conducting tutorials and workshops, Grading assignments and exams, Holding\\noffice hours\\n•Assisting with programming labs: Support students during programming labs or coding sessions, providing\\nguidance on coding best practices, troubleshooting errors, and assisting with debugging. Help students gain\\nhands-on experience with Python, C++, or Deep Learning frameworks and tools.\\n•Providing technical support: Assist students with technical issues related to programming environments, software\\ninstallations, or any other technical difficulties they may encounter while working on course assignments or\\nprojects,Facilitating group discussions and collaborations, Keeping up with course updates\\nSoftware Testing Engineer Jan 2020 - Dec 2020\\nTATA Consultancy Services Ltd., Indore, India\\n•Helios Quota Recon Built a tool that helped my fellow employees at TCS, automate Quota Reconciliations.\\n•Used to Analyze the data from two different DB and see whether the data was equating to the same value\\n•Improved the teams processing by three-fourth times\\n•Used Frameworks like PowerAutomate and ,Visual Studio from python for the developement and used as an\\n.exe file\\nInformation Processing Specialist May 2019 - Dec 2019\\nTATA Consultancy Services Ltd., Indore, India\\n•Facilitated roll out applications for HR management in ServiceNow helped team in developing a PowerAutomate\\nApplication the made things easier in Daily Activities using Visual Studio\\n•Used to Analyze the data from two different DB and see whether the data was equating to the same value\\n•Used SQL Server to match the details on two different Databases.\\n•Attending meetings and training for providing better support with the client\\nPROJECTS1Classification of botnets using CNN. Research on Classification of Botnets using CNN using N-BaIoT Dataset.\\n[Accepted for IEEE IPCCC Conference on November,2023] (April 2023)\\n•Analysis of Botnet Attacks on Nine Different IoT Devices.\\n•Developed a Classification model with high accuracy using various machine learning models like Decision Tree,\\nRandom Forest, k-Neighbors etc.,\\n•Developed Convolutional Neural Networks on Continuous Network Data for Classifying 9 Different types of\\nAttacks.\\n•Used Frameworks like Keras, Tensorflow from python for the developement and seaborn,matlpolib for the analysis\\nand visualization\\n2Forest Fire Detection using IoT Used an ARM BOARD to detect forest fires using temperature and humidity\\nsensors and connected it to google cloud for their results. (December 2022)\\n•Trained a machine learning model that uses gradient boosting classifier which predicts whether ther is fire or\\nnot using the Canadian institute forest fire dataset\\n•Used a GCP Cloud to connect the edge board that sense the temperature, humidity, and precipitation values\\n•Used C++ in Arm MbedStudio to send the sensor data to GCP from DISCO Board\\n3A Review for detecting Botnets using WSN Dataset. A Research paper on the survey of detecting botnets\\nusing various ML methods on WSN Dataset. (April 2022)\\n4Detecting Botnets using WSN Dataset. Built a ML project that predicts whether a sensor network is attacked\\nby a botnet or not. (April 2022)\\n•Analysis of Botnet Attacks on Wireless Sensor Networks.\\n•Developed and compared a Classification model with excellent accuracy utilizing multiple machine learning\\nmethods such as Decision Tree, Random Forest, and k-Neighbors.\\n5Predicting Diabetes in Pregnant Women. A machine learning study that predicts if a pregnant woman is likely\\nto develop diabetes based on her age, glucose levels, BMI, and other factors. (March 2022)\\n•Analysis of Diabetes found on Pregnant Women using Python, pandas, matplotlib and seaborn\\n•Developed a high-accuracy classification model employing several machine learning models such as Decision\\nTree, Random Forest, k-Neighbors, and Support Vector Machines.\\n•To obtain the best parameters of the best model, Randomized Search Cross-Validation Techniques were used.\\n•Frameworks such as Keras and Tensorflow from Python were used for development, and seaborn and matlpolib\\nwere used for analysis and visualization.\\n6Analyzing Crashes Severity During Extreme Weather Conditions A Data Analysis project that analyzes\\nthe weather conditions and their impact on traffic severity. (May 2023)\\n•Performed Data Cleaning, Data Modeling, Feature Selection and Analysis using different libraries of Python\\n•Visualisation of the Analysis that was done using libraries like plotly, seaborn, matplotlib etc.,\\n7Healthcare-Go. Build an application that reminds old people about their medical activities using Android Studio\\nand Firebase. (May 2019)\\n•An Android application developed in Android Studio using Java\\n•Has a BMI Calculator and also an user defined reminder list that reminds according to their settings\\n•An on-the-go Framework that helps the patients to connect with doctors.\\nACHIEVEMENTS\\n•Best Employee, Tata Consultancy Services, May 2020 - July 2020.\\n•Top 5 in Hackathon conducted by DevmenIT on Microsoft applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_list = nltk.sent_tokenize(text)\n",
        "sentence_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kX11kkWOOWA",
        "outputId": "4bfdc7f8-a3fe-4d5c-d03a-846f987c8547"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DHATRI SAI KUMAR REDDY GUDIPELLY\\n+1(858) 923-3101 ⋄San Marcos, CA\\ndskreddy15@gmail.com ⋄linkedin.com/dhatrisaireddy ⋄github.com/dskreddy15\\nOBJECTIVE\\nLooking for a full-time position in a competitive setup where one will be able to contribute to the firm’s progress\\nsubstantially and learn new skills.',\n",
              " 'Eager to apply research work to industry data and continue innovating and\\ndeveloping new ideas.',\n",
              " 'EDUCATION\\nMaster of Computer Science , California State University San Marcos 2023\\nRelevant Coursework: Machine Learning, Advance Database Management Systems, Artificial Intelligence, and Data\\nMining.',\n",
              " 'SKILLS\\nLanguages Python, JavaScript, SQL, C/C++, R\\nFrameworks ReactJs, Spring Boot, AngularJs, NodeJs, Tensorflow, Scikit\\nTools GIT, PostgreSQL, MySQL, Sonarqube, Jupyter, Visual Studio\\nPlatforms Linux, Windows, Raspberry, AWS, GCP, arm Mbed, MATLAB\\nSkills Research, AI/ML,Python, SQL, Mathematics, Data Mining, Data Analysis, Statistics.',\n",
              " 'EXPERIENCE\\nStudent Assistant Aug 2021 - May 2023\\nCalifornia State University San Marcos, CA\\n•Assisting with course preparation, Conducting tutorials and workshops, Grading assignments and exams, Holding\\noffice hours\\n•Assisting with programming labs: Support students during programming labs or coding sessions, providing\\nguidance on coding best practices, troubleshooting errors, and assisting with debugging.',\n",
              " 'Help students gain\\nhands-on experience with Python, C++, or Deep Learning frameworks and tools.',\n",
              " '•Providing technical support: Assist students with technical issues related to programming environments, software\\ninstallations, or any other technical difficulties they may encounter while working on course assignments or\\nprojects,Facilitating group discussions and collaborations, Keeping up with course updates\\nSoftware Testing Engineer Jan 2020 - Dec 2020\\nTATA Consultancy Services Ltd., Indore, India\\n•Helios Quota Recon Built a tool that helped my fellow employees at TCS, automate Quota Reconciliations.',\n",
              " '•Used to Analyze the data from two different DB and see whether the data was equating to the same value\\n•Improved the teams processing by three-fourth times\\n•Used Frameworks like PowerAutomate and ,Visual Studio from python for the developement and used as an\\n.exe file\\nInformation Processing Specialist May 2019 - Dec 2019\\nTATA Consultancy Services Ltd., Indore, India\\n•Facilitated roll out applications for HR management in ServiceNow helped team in developing a PowerAutomate\\nApplication the made things easier in Daily Activities using Visual Studio\\n•Used to Analyze the data from two different DB and see whether the data was equating to the same value\\n•Used SQL Server to match the details on two different Databases.',\n",
              " '•Attending meetings and training for providing better support with the client\\nPROJECTS1Classification of botnets using CNN.',\n",
              " 'Research on Classification of Botnets using CNN using N-BaIoT Dataset.',\n",
              " '[Accepted for IEEE IPCCC Conference on November,2023] (April 2023)\\n•Analysis of Botnet Attacks on Nine Different IoT Devices.',\n",
              " '•Developed a Classification model with high accuracy using various machine learning models like Decision Tree,\\nRandom Forest, k-Neighbors etc.,\\n•Developed Convolutional Neural Networks on Continuous Network Data for Classifying 9 Different types of\\nAttacks.',\n",
              " '•Used Frameworks like Keras, Tensorflow from python for the developement and seaborn,matlpolib for the analysis\\nand visualization\\n2Forest Fire Detection using IoT Used an ARM BOARD to detect forest fires using temperature and humidity\\nsensors and connected it to google cloud for their results.',\n",
              " '(December 2022)\\n•Trained a machine learning model that uses gradient boosting classifier which predicts whether ther is fire or\\nnot using the Canadian institute forest fire dataset\\n•Used a GCP Cloud to connect the edge board that sense the temperature, humidity, and precipitation values\\n•Used C++ in Arm MbedStudio to send the sensor data to GCP from DISCO Board\\n3A Review for detecting Botnets using WSN Dataset.',\n",
              " 'A Research paper on the survey of detecting botnets\\nusing various ML methods on WSN Dataset.',\n",
              " '(April 2022)\\n4Detecting Botnets using WSN Dataset.',\n",
              " 'Built a ML project that predicts whether a sensor network is attacked\\nby a botnet or not.',\n",
              " '(April 2022)\\n•Analysis of Botnet Attacks on Wireless Sensor Networks.',\n",
              " '•Developed and compared a Classification model with excellent accuracy utilizing multiple machine learning\\nmethods such as Decision Tree, Random Forest, and k-Neighbors.',\n",
              " '5Predicting Diabetes in Pregnant Women.',\n",
              " 'A machine learning study that predicts if a pregnant woman is likely\\nto develop diabetes based on her age, glucose levels, BMI, and other factors.',\n",
              " '(March 2022)\\n•Analysis of Diabetes found on Pregnant Women using Python, pandas, matplotlib and seaborn\\n•Developed a high-accuracy classification model employing several machine learning models such as Decision\\nTree, Random Forest, k-Neighbors, and Support Vector Machines.',\n",
              " '•To obtain the best parameters of the best model, Randomized Search Cross-Validation Techniques were used.',\n",
              " '•Frameworks such as Keras and Tensorflow from Python were used for development, and seaborn and matlpolib\\nwere used for analysis and visualization.',\n",
              " '6Analyzing Crashes Severity During Extreme Weather Conditions A Data Analysis project that analyzes\\nthe weather conditions and their impact on traffic severity.',\n",
              " '(May 2023)\\n•Performed Data Cleaning, Data Modeling, Feature Selection and Analysis using different libraries of Python\\n•Visualisation of the Analysis that was done using libraries like plotly, seaborn, matplotlib etc.,\\n7Healthcare-Go.',\n",
              " 'Build an application that reminds old people about their medical activities using Android Studio\\nand Firebase.',\n",
              " '(May 2019)\\n•An Android application developed in Android Studio using Java\\n•Has a BMI Calculator and also an user defined reminder list that reminds according to their settings\\n•An on-the-go Framework that helps the patients to connect with doctors.',\n",
              " 'ACHIEVEMENTS\\n•Best Employee, Tata Consultancy Services, May 2020 - July 2020.',\n",
              " '•Top 5 in Hackathon conducted by DevmenIT on Microsoft applications.']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greeting_response(text):\n",
        "  text = text.lower()\n",
        "\n",
        "  bot_greetings = ['Hello', 'Hi', 'Hey, There']\n",
        "\n",
        "  user_greetings = ['Hi', 'Hello', 'Hey', 'greetings', 'wassup', 'Whats up?']\n",
        "\n",
        "  for word in text.split():\n",
        "    if word in user_greetings:\n",
        "      return random.choice(bot_greetings)"
      ],
      "metadata": {
        "id": "MCltlR5EOuVQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def index_sort(list_var):\n",
        "  length = len(list_var)\n",
        "  list_index = list(range(0,length))\n",
        "\n",
        "  x = list_var\n",
        "  for i in range(length):\n",
        "    for j in range(length):\n",
        "      if x[list_index[i]] > x[list_index[j]]:\n",
        "        temp = list_index[i]\n",
        "        list_index[i] = list_index[j]\n",
        "        list_index[j] = temp\n",
        "\n",
        "  return list_index"
      ],
      "metadata": {
        "id": "X_R-eXccYVKe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bot_response(user_input):\n",
        "  user_input = user_input.lower()\n",
        "  sentence_list.append(user_input)\n",
        "  bot_response = ''\n",
        "  cm = CountVectorizer().fit_transform(sentence_list)\n",
        "  similarity_scores = cosine_similarity(cm[-1],cm)\n",
        "  similarity_scores_list = similarity_scores.flatten()\n",
        "  index = index_sort(similarity_scores_list)\n",
        "  index = index[1:]\n",
        "  response_flag = 0\n",
        "\n",
        "  j = 0\n",
        "  for i in range(len(index)):\n",
        "    if similarity_scores_list[index[i]] > 0.0:\n",
        "      bot_response = bot_response+' '+sentence_list[index[i]]\n",
        "      response_flag = 1\n",
        "      j = j+1\n",
        "    if j > 2:\n",
        "      break\n",
        "\n",
        "    if response_flag == 0:\n",
        "      bot_response = bot_response+' '+\"I'll Let Dhatri Know, He needs to improve his skills\"\n",
        "\n",
        "    sentence_list.remove(user_input)\n",
        "\n",
        "    return bot_response\n"
      ],
      "metadata": {
        "id": "XqBo72vcQIhY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('More Info on Dhatri, Enter else Say Bye')\n",
        "\n",
        "exit_list = ['exit', 'ok','Thanks','Bye','See you']\n",
        "while True:\n",
        "  user_input = input()\n",
        "  if user_input.lower() in exit_list:\n",
        "    print('Thanks You for your time.')\n",
        "    break\n",
        "  else:\n",
        "    if greeting_response(user_input) != None:\n",
        "      print('bot: '+greeting_response(user_input))\n",
        "    else:\n",
        "      print('bot: '+bot_response(user_input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO_UdX_RdVSb",
        "outputId": "4841d570-600e-4a0a-c303-872dc7cf5bcd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "More Info on Dhatri, Enter else Say Bye\n",
            "Hello How are you doing\n",
            "bot:  I'll Let Dhatri Know, He needs to improve his skills\n",
            "Education\n",
            "bot:  EDUCATION\n",
            "Master of Computer Science , California State University San Marcos 2023\n",
            "Relevant Coursework: Machine Learning, Advance Database Management Systems, Artificial Intelligence, and Data\n",
            "Mining.\n",
            "Experience\n",
            "bot:  Help students gain\n",
            "hands-on experience with Python, C++, or Deep Learning frameworks and tools.\n",
            "EXPERIENCE\n",
            "bot:  Help students gain\n",
            "hands-on experience with Python, C++, or Deep Learning frameworks and tools.\n",
            "Python\n",
            "bot:  SKILLS\n",
            "Languages Python, JavaScript, SQL, C/C++, R\n",
            "Frameworks ReactJs, Spring Boot, AngularJs, NodeJs, Tensorflow, Scikit\n",
            "Tools GIT, PostgreSQL, MySQL, Sonarqube, Jupyter, Visual Studio\n",
            "Platforms Linux, Windows, Raspberry, AWS, GCP, arm Mbed, MATLAB\n",
            "Skills Research, AI/ML,Python, SQL, Mathematics, Data Mining, Data Analysis, Statistics.\n",
            "PROJECTS\n",
            "bot:  •Providing technical support: Assist students with technical issues related to programming environments, software\n",
            "installations, or any other technical difficulties they may encounter while working on course assignments or\n",
            "projects,Facilitating group discussions and collaborations, Keeping up with course updates\n",
            "Software Testing Engineer Jan 2020 - Dec 2020\n",
            "TATA Consultancy Services Ltd., Indore, India\n",
            "•Helios Quota Recon Built a tool that helped my fellow employees at TCS, automate Quota Reconciliations.\n",
            "ACHIEVEMENTS\n",
            "bot:  ACHIEVEMENTS\n",
            "•Best Employee, Tata Consultancy Services, May 2020 - July 2020.\n",
            "bye\n",
            "bot:  I'll Let Dhatri Know, He needs to improve his skills\n",
            "exit\n",
            "Thanks You for your time.\n"
          ]
        }
      ]
    }
  ]
}